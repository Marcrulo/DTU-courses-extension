{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stored course-site HTMLs\n",
    "with open('valid_courses.json') as f:\n",
    "    valid_courses = json.load(f)\n",
    "\n",
    "# Load department color mapping\n",
    "with open('department_colors.json') as f:\n",
    "    department_colors = json.load(f)\n",
    "\n",
    "# Load department names\n",
    "with open('department_names.json') as f:\n",
    "    department_names = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph for the project consist of the data from DTU's webpage: https://kurser.dtu.dk/.\n",
    "\n",
    "The initial attributes of the graph are:\n",
    "\n",
    "* **course_num**: Course number\n",
    "* **page**: HTML page\n",
    "* **department**: Number of DTU department for the course\n",
    "* **department_colors**: Colour of the corresponding department\n",
    "* **department_names**: Name of DTU department for the course\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directd graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Go through each course\n",
    "for course_num in valid_courses:\n",
    "    department = course_num[:2] \n",
    "    G.add_node(course_num,\n",
    "               course_num=course_num,\n",
    "               page=valid_courses[course_num],\n",
    "               department=department, \n",
    "               color=department_colors[department],\n",
    "               department_name=department_names[department])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edges are directed and are defined by the prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course_num in valid_courses:\n",
    "\n",
    "    # Initialize BeautifulSoup object\n",
    "    page = G.nodes[course_num]['page']\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    # Define the search pattern to match both \"Academic prerequisites\" and \"Mandatory prerequisites\"\n",
    "    search_pattern = r\"(Academic prerequisites|Mandatory Prerequisites)\"\n",
    "    \n",
    "    # Find the label element that matches the pattern\n",
    "    label = soup.find('label', string=re.compile(search_pattern))\n",
    "    if label is None:\n",
    "        continue  # Skip if no label is found\n",
    "    \n",
    "    # Get the parent element that contains the label and prerequisites\n",
    "    parent = label.find_parent().find_parent()\n",
    "    \n",
    "    # Get the second <td> (assuming it contains the prerequisites text)\n",
    "    prerequisite = parent.find_all('td')[1].text\n",
    "\n",
    "    # Remove whitespace and line breaks\n",
    "    prerequisite = prerequisite.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    # Extract 5-digit course numbers\n",
    "    prerequisites = set(re.findall(r'\\d{5}', prerequisite))\n",
    "    \n",
    "    # Add edges to the graph for valid prerequisites\n",
    "    for prerequisite in prerequisites:\n",
    "        if prerequisite in G.nodes:\n",
    "            if prerequisite != course_num:  # Skip self-loops\n",
    "                G.add_edge(prerequisite, course_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add text attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes are added:\n",
    "\n",
    "* **course_title**: Title of the course\n",
    "* **course_text**: Cleaned text of the description from webpage\n",
    "* **text_size**: Number of characters in the description\n",
    "* **word_count**: Number of words in the description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ../id_to_name.json\n"
     ]
    }
   ],
   "source": [
    "id_to_name = {}\n",
    "for node in G.nodes:\n",
    "    \n",
    "    ### Initialize BeuatifulSoup object\n",
    "    page = G.nodes[node]['page']\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    ### Add title to node\n",
    "    title = soup.title.text\n",
    "    cleaned = ' '.join(title.strip().split())[6:]\n",
    "    G.nodes[node]['course_title'] = cleaned\n",
    "    id_to_name[node] = cleaned\n",
    "    \n",
    "    ### Add course text to node (General course objectives + Learning objectives + Content)\n",
    "    div = soup.find('div', string=re.compile(\"General course objectives\")).parent(string=True)\n",
    "    \n",
    "    remove_indeces = []\n",
    "    for d, text in enumerate(div):\n",
    "        if text in [\"General course objectives\", \"Learning objectives\", \"Content\", \"Last updated\", \"\\r\\nA student who has met the objectives of the course will be able to:\\r\\n\\r\\n\"]:\n",
    "            remove_indeces.append(d)\n",
    "    \n",
    "    new_div = [div[i] for i in range(len(div)) if i not in remove_indeces]\n",
    "    text = ' '.join(new_div[:-1]).replace('\\r', ' ').replace('\\n', '')\n",
    "    cleaned = ' '.join(text.strip().split())\n",
    "    G.nodes[node]['course_text'] = cleaned  \n",
    "    G.nodes[node]['text_size'] = len(cleaned) \n",
    "    G.nodes[node]['word_count'] = len(cleaned.split())\n",
    "\n",
    "    \n",
    "# delete ../graphs.json\n",
    "file_path = Path(\"../id_to_name.json\")\n",
    "if file_path.exists():\n",
    "    os.remove(file_path)\n",
    "    print(f\"Deleted {file_path}\")\n",
    "    \n",
    "with open('../id_to_name.json', 'w') as f:\n",
    "    json.dump(id_to_name, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course dependencies overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_to_json(center_node, G, levels, filename=\"../graphs.json\"):\n",
    "    \n",
    "    # get maximum number of nodes that are at the same level. One for level<0 and one for level>0\n",
    "    subseq_height = defaultdict(int)\n",
    "    prereq_height = defaultdict(int)\n",
    "\n",
    "    for level in levels.values():\n",
    "        if level > 0:\n",
    "            subseq_height[level] += 1\n",
    "        elif level < 0:\n",
    "            prereq_height[abs(level)] += 1\n",
    "\n",
    "    # Convert graph to the desired dict format\n",
    "    graph_data = {\n",
    "        \"nodes\": [{\"id\": str(n), \"level\": levels.get(n, 0)} for n in G.nodes()],\n",
    "        \"edges\": [{\"source\": str(u), \"target\": str(v)} for u, v in G.edges()],\n",
    "        \"max_subseq\": max(levels.values(), default=0),\n",
    "        \"max_prereq\": abs(min(levels.values(), default=0)),\n",
    "        \"subseq_height\": max(subseq_height.values(), default=0),\n",
    "        \"prereq_height\": max(prereq_height.values(), default=0)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Ensure file exists (with empty dict if not)\n",
    "    file_path = Path(filename)\n",
    "    if file_path.exists():\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "    \n",
    "    # Add or overwrite this graph entry\n",
    "    data[str(center_node)] = graph_data\n",
    "    \n",
    "    # Write back to file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# # Example usage:\n",
    "# G = nx.Graph()\n",
    "# G.add_node(\"01002\", name=\"Math\")\n",
    "# G.add_node(\"01003\")\n",
    "# G.add_node(\"01567\")\n",
    "# G.add_edges_from([(\"01003\", \"01002\"), (\"01002\", \"01567\")])\n",
    "# graph_to_json(\"01002\", G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ../graphs.json\n"
     ]
    }
   ],
   "source": [
    "# delete ../graphs.json\n",
    "import os\n",
    "file_path = Path(\"../graphs.json\")\n",
    "if file_path.exists():\n",
    "    os.remove(file_path)\n",
    "    print(f\"Deleted {file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Get all courses\n",
    "all_course_nums = list(G.nodes())\n",
    "# center_node = all_course_nums[100]\n",
    "\n",
    "for center_node in all_course_nums:\n",
    "\n",
    "    # Get forward and reverse BFS nodes\n",
    "    forward_nodes = nx.single_source_shortest_path_length(G, center_node)\n",
    "    reverse_nodes = nx.single_source_shortest_path_length(G.reverse(copy=False), center_node)\n",
    "\n",
    "    # Combine all relevant nodes\n",
    "    all_nodes = set(forward_nodes.keys()) | set(reverse_nodes.keys())\n",
    "    levels = {}\n",
    "\n",
    "    for node in all_nodes:\n",
    "        if node == center_node:\n",
    "            levels[node] = 0\n",
    "        elif node in reverse_nodes:\n",
    "            levels[node] = -reverse_nodes[node]\n",
    "        else:\n",
    "            levels[node] = forward_nodes[node]\n",
    "\n",
    "    # Filter edges: only include edges between different levels\n",
    "    # edges = [\n",
    "    #     (u, v) for u, v in G.subgraph(all_nodes).edges()\n",
    "    #     if levels[u] != levels[v]\n",
    "    # ]\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    # Create a new DiGraph with only those edges\n",
    "    filtered_subG = nx.DiGraph()\n",
    "    filtered_subG.add_nodes_from(all_nodes)\n",
    "    \n",
    "\n",
    "    # Build layout positions\n",
    "    pos = {}\n",
    "    level_nodes = {}\n",
    "    for node, level in levels.items():\n",
    "        level_nodes.setdefault(level, []).append(node)\n",
    "\n",
    "    for level in sorted(level_nodes):\n",
    "        nodes = level_nodes[level]\n",
    "        for i, node in enumerate(nodes):\n",
    "            pos[node] = (level, -i)\n",
    "\n",
    "    edges = [\n",
    "        (u, v) for u, v in G.subgraph(all_nodes).edges()\n",
    "        if pos[u][0] < pos[v][0]\n",
    "    ]\n",
    "    filtered_subG.add_edges_from(edges)\n",
    "\n",
    "    # Write to JSON\n",
    "    graph_to_json(center_node, filtered_subG, levels)\n",
    "\n",
    "# Plot\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# nx.draw(filtered_subG, pos, with_labels=True, node_color=\"lightgreen\", node_size=1000, arrows=True)\n",
    "# plt.title(f\"Layered graph centered on {center_node} (no intra-layer arrows)\")\n",
    "# plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/20037122/draw-an-arrow-between-two-divs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
